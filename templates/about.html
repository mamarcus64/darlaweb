<div class="container">
  
  <h2>FAQs</h2>

  
  <ol class="list-group">
    <li class="list-group-item">
    <a href="#" data-toggle="collapse" data-target="#whatbody"><h3
    id="what">
    What is DARLA?</h3></a>

    <div id="whatbody" class="collapse">
      <p>DARLA is a web application providing two main functionalities for vowel
    extraction from speech: completely
    automated and semi-automated.</p>
    
<p>The completely automated system transcribes the input
    speech data using automatic speech recognition (ASR), and then
    runs it through forced alignment and formant extraction.</p>
    
    <p>The semi-automated system is a <a
    href="http://fave.ling.upenn.edu/">FAVE-based</a> approach that
    aligns and extracts vowel formants from speech with manual
      transcriptions.</p></div>
      </li>

      <li class="list-group-item">

      <a href="#" data-toggle="collapse" data-target="#outputbody"><h3 id="output">What is the system's output?</h3></a>

       <div id="outputbody" class="collapse">
<p>We e-mail a set of files to you: a vowel plot showing the mean of stressed vowels
after filtering out grammatical function words, a spreadsheet with
both unnormalized and Lobanov-normalized formant measurements
(pre-filtered to remove grammatical function words and tokens with
high bandwidths for F1 or F2), the same spreadsheet formatted for
convenient uploading to <a
href="http://lvc.uoregon.edu/norm/about_norm1.php">NORM</a>, the
alignments, and the transcriptions. </p>

<p>If you are using the completely automated feature, you also have the option of
editing the transcriptions online, and seamlessly re-running your task
	 through the system.</p>
       </div>
       
    </li>
    
    <li class="list-group-item">
    <a href="#" data-toggle="collapse" data-target="#asrbody">
    <h3 id="asr">What ASR do you use for the completely automated feature?</h3></a>

     <div id="asrbody" class="collapse">
    <p>We give you two options: our in-house ASR, and YouTube's
    closed captioning.</p>

    <ul>
      <li>
      <p>The in-house ASR is a research system built with the <a
    href="http://cmusphinx.sourceforge.net/">CMU Sphinx</a> toolkit.
    It is the fastest method, and works reasonably well for some kinds
    of recordings.
    However, the results often have a fair number of transcription
      errors, since we don't have the resources to match the commercial
   state-of-the-art.</p>
      
    <p>For this reason, we have added a convenient option to make manual
    corrections: the initial results e-mailed to you 
    include a link to an online playback tool which divides your
    recording and ASR transcription into 20-second audio chunks that
    can be corrected and then directly resubmitted to DARLA. </p></li>
    
<li> The other option uses Google's state-of-the-art ASR through
    YouTube's Closed Captioning. Your audio is uploaded to YouTube (to a private secure
    account), and the transcriptions are extracted.</p>

      <p>One drawback is that YouTube often takes 5+ hours to
    produce captions. (The transcriptions are typically quite good,
    so it's worth the wait.) Note that you don't have to upload anything to
    YouTube or convert your audio into a video yourself; DARLA takes care of
    that for you! See the <a href="cave">system page</a> for details.</p>
 
    
  
    </li>
  </ul>
     </div></li>

       <li class="list-group-item">
  <a href="#eval" data-toggle="collapse" data-target="#evalbody">
      <h3 id="eval">How can I evaluate the accuracy of the ASR transcriptions?</h3></a>

  <div id="evalbody" class="collapse">
    <p>
    <a href="uploadeval">Our online transcription evaluation tool</a>
    uses the weighted Levenshtein distance algorithm to compute
   transcription error rates for words, phonemes, and stressed vowels. Simply
    upload the ASR transcription and the manual transcription in
    plaintext format.</p>
    
</div></li>


     
  <li class="list-group-item">
  <a href="#tech" data-toggle="collapse" data-target="#techbody">
      <h3 id="tech">What do you use for alignment and extraction?</h3></a>

  <div id="techbody" class="collapse">
<p>We use the same methods as FAVE. Our forced alignment is done with the
  <a href="http://prosodylab.org/tools/aligner/">ProsodyLab
  Aligner</a>, and formant measurement with <a
  href="https://github.com/JoFrhwld/FAVE/tree/master/FAVE-extract">FAVE-extract</a>.
  We also use the <a href="http://cran.r-project.org/web/packages/vowels/index.html">R vowels package</a> for plotting.</p>
</div></li>

    <li class="list-group-item">
    <a href="#why" data-toggle="collapse" data-target="#whybody">
    <h3 id="why">Why completely automate vowel extraction?</h3>
    </a>

     <div class="collapse" id="whybody">
<p>In the last few years, sociolinguists have begun using semi-automated speech processing methods such as Penn's <a href="http://fave.ling.upenn.edu/index.html">FAVE</a> program to extract vowel formants. These systems have accelerated the pace of linguistic research, but require significant human effort to manually create sentence-level transcriptions. </p>

<p>We believe that sociolinguistics is on the brink of an even more
  transformative technology: large-scale, completely automated vowel
  extraction without any need for human transcription. This technology
  would make it possible to quickly extract pronunciation features
  from hours of recordings, including YouTube and vast audio
    archives. DARLA takes a step in this direction. </p>
  </div>
  
      </li>
      
      <li class="list-group-item">
<a href="#errors" data-toggle="collapse" data-target="#errorsbody">
      <h3 id="errors">How do we deal with ASR errors?</h3></a>

      <div class="collapse" id="errorsbody">
<p>While ASR is far from perfect, we believe sociophoneticians do not need to wait for years to take advantage of speech recognition. Unlike applications like dictation software where accurate word recognition is the primary goal, sociophonetics typically focuses on a much narrower objective: extracting a representative vowel space for speakers, based on stressed vowel tokens.
For example, it would usually not be crucial to know that the stressed vowel in the word "turning" was extracted from "turn it" rather than "turning", or that "tack" was wrongly transcribed as "sack".  Such differences will have little effect on the speaker's vowel space for many sociophonetic questions. </p>

<p>It turns out that most ASR errors affect the identity of the <i>words</i> but not the identity of the <i>vowels</i> (especially stressed vowels), making it an ideal technology for automated vowel analysis. 
Of course, there will be instances of vowel error, but the effect of these errors is reduced by the large amount of data with hundreds or thousands of vowel tokens.</p>

<p>Important contrasts like "cot" versus "caught" tend to be handled by ASR's modeling of grammatical plausibility (using a <a href="http://en.wikipedia.org/wiki/Language_model">language model</a>). The system would be unlikely to transcribe "I caught the ball" as "I cot the ball" since the latter would be improbable under an English language model.</p>

<p>Finally, since DARLA shows probabilities for the phonetic
	environment around each vowel (e.g., obstruent+vowel+nasal
	consonant), researchers can examine contrasts like pin/pen
	versus pit/pet.</p>
      </div>
      </li>
      
      <li class="list-group-item">

      <a href="#limits" data-toggle="collapse" data-target="#limitsbody">
      <h3 id="limits">Sounds great! What's the catch?</h3></a>

       <div class="collapse" id="limitsbody">  
<p> DARLA's completely automated system cannot provide perfect transcriptions. The automatic
transcriptions typically contain a very large number of errors,
especially in free speech data, even using YouTube's ASR. Current technology simply cannot match the accuracy of human manual transcriptions.</p>

<p>DARLA's automated approach to sociophonetics may help open the way
toward large-scale audio analysis, but there is a tradeoff. As with
many other sciences, automated processing necessitates error-reporting
in the measurements, not just in the statistical modeling. We find
that the system can be useful for extracting a representative vowel
	 space for sociophonetic purposes, as long as error levels are
	 considered and reported. In other words, fast large-scale
	 data analysis requires a higher tolerance of noise in the
	 data.</p>
       </div>
       
       </li>
       

       <li class="list-group-item">
       <a href="#semi" data-toggle="collapse" data-target="#semibody">
       <h3 id="semi">What is the semi-automated functionality?</h3></a>

       <div class="collapse" id="semibody">  
<p>This is designed for research that requires accurate human
transcription.
It builds upon <a href="http://fave.ling.upenn.edu/index.html">FAVE</a>, but with a
different interface and output options.</p>

<p>Our system allows you to upload your transcriptins in different
formats: as plaintext files, or as a TextGrid with a pair of
boundaries around each transcribed sentence (the "Boundaries" option).
You can also upload manually aligned/corrected TextGrids for formant
	 extraction only.</p>


<p> For research requiring perfect transcriptions, we recommend either
using either the "Boundaries" option OR using the
YouTubeClosedCaptions ASR as a first pass and then correcting it
online with our playback tool. The plaintext
method works just as well, but you will need to delete noises,
laughter, interviewer's voice, etc. With the Boundaries method, such
deletions aren't necessary since you are simply putting boundaries
around the parts of the recording that you want.</p>

       </div>

            <li class="list-group-item">

      <a href="#noise" data-toggle="collapse" data-target="#noisebody">
      <h3 id="noise">What about noise or multiple voices in the recording?</h3></a>

       <div class="collapse" id="noisebody">

	 <p>The system cannot currently handle recordings with
	 multiple speakers in an automated way (though this is an idea
	 for future work). ASR and forced alignment also have trouble with noise, laughter, loud breaths, background
	 voices, music, etc. </p>

	<p> For this reason, we recommend that you "pre-clean" the
	recording by manually deleting extraneous sounds and
	voices. This is easy to do in Praat (select the noise and
	click Cmd+X or Ctrl+X). </p>

	<p>When DARLA processes a recording with such noise, the
      alignment is likely to be incorrect, as the system will wrongly think that it should align those noises with words from the transcription. If your recording would require a great deal of pre-cleaning, you might want to consider manually transcribing with the semi-automated method rather than the completely automated one.</p>
      </div>

      
    </ul>   <!-- ends list-group ul -->
    
    

</div>   <!-- ends container -->
